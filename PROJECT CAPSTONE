import os
import sys
import re
import shutil
import time
import pickle
from collections import defaultdict
import numpy as np
import pandas as pd
from elasticsearch import Elasticsearch
from elasticsearch import helpers, Elasticsearch
from elasticsearch.helpers import bulk
from datetime import datetime
import elasticsearch
import datetime
import csv
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

COLORS = ["slategrey","darkorange","royalblue"]

def fetch_data(data_dir):

   
    if not os.path.isdir(data_dir):
        raise Exception("specified data dir does not exist")
    if not len(os.listdir(data_dir)) > 0:
        raise Exception("specified data dir does not contain any files")

    file_list = [os.path.join(data_dir,f) for f in os.listdir(data_dir) if re.search("\.json",f)]
    correct_columns = ["country", "customer_id", "day", "invoice", "month",
                       "price", "stream_id", "times_viewed", "year"]

    ##############################
    all_months = {}
    for file_name in file_list:
        data = pd.read_json(file_name)
        all_months[os.path.split(file_name)[-1]] = data

    #format data with correct column names 
    for f,data in all_months.items():
        cols = set(data.columns.tolist())
        if 'total_price' in cols:
            data.rename(columns={'total_price':'price'},inplace=True)
        if 'TimesViewed' in cols:
            data.rename(columns={'TimesViewed':'times_viewed'},inplace=True)
        if 'StreamID' in cols:
             data.rename(columns={'StreamID':'stream_id'},inplace=True)
        

        cols = data.columns.tolist()
        if sorted(cols) != correct_columns:
            raise Exception("columns name could not be matched to correct cols")

    #data concatenation 
    data = pd.concat(list(all_months.values()),sort=True)
    years,months,days = data['year'].values,data['month'].values,data['day'].values 
    dates = ["{}-{}-{}".format(years[i],str(months[i]).zfill(2),str(days[i]).zfill(2)) for i in range(data.shape[0])]
    data['invoice_date'] = np.array(dates,dtype='datetime64[D]')
    data['invoice'] = [re.sub("\D+","",i) for i in data['invoice'].values]
    
    ## sort by date and reset the index
    data.sort_values(by='invoice_date',inplace=True)
    data.reset_index(drop=True,inplace=True)
    
    return(data)


def convert_to_ts(data_orig, country=None):
   

    if country:
        if country not in np.unique(df_orig['country'].values):
            raise Excpetion("country not found")
    
        mask = data_orig['country'] == country
        data = data_orig[mask]
    else:
        data = data_orig
        
    ## use a date range to ensure all days are accounted for in the data
    invoice_dates = df['invoice_date'].values
    start_month = '{}-{}'.format(df['year'].values[0],str(data['month'].values[0]).zfill(2))
    stop_month = '{}-{}'.format(df['year'].values[-1],str(data['month'].values[-1]).zfill(2))
    data_dates = data['invoice_date'].values.astype('datetime64[D]')
    days = np.arange(start_month,stop_month,dtype='datetime64[D]')
    
    purchases = np.array([np.where(df_dates==day)[0].size for day in days])
    invoices = [np.unique(data[data_dates==day]['invoice'].values).size for day in days]
    streams = [np.unique(data[data_dates==day]['stream_id'].values).size for day in days]
    views =  [data[data_dates==day]['times_viewed'].values.sum() for day in days]
    revenue = [data[data_dates==day]['price'].values.sum() for day in days]
    year_month = ["-".join(re.split("-",str(day))[:2]) for day in days]

    data_time = pd.DataFrame({'date':days,
                            'purchases':purchases,
                            'unique_invoices':invoices,
                            'unique_streams':streams,
                            'total_views':views,
                            'year_month':year_month,
                            'revenue':revenue})
    return(data_time)


def fetch_ts(data_dir, clean=False):
   
    #convenience function to read in new data
    
  

    ts_data_dir = os.path.join(data_dir,"ts-data")
    
    if clean:
        shutil.rmtree(ts_data_dir)
    if not os.path.exists(ts_data_dir):
        os.mkdir(ts_data_dir)

         
    if len(os.listdir(ts_data_dir)) > 0:
        print("... loading ts data from files")
        return({re.sub("\.csv","",cf)[3:]:pd.read_csv(os.path.join(ts_data_dir,cf)) for cf in os.listdir(ts_data_dir)})

    ## get original data
    print("..... processing data")
    df = fetch_data(data_dir)

    ## find the top ten countries (wrt revenue)
    table = pd.pivot_table(df,index='country',values="price",aggfunc='sum')
    table.columns = ['total_revenue']
    table.sort_values(by='total_revenue',inplace=True,ascending=False)
    top_ten_countries =  np.array(list(table.index))[:10]

    file_list = [os.path.join(data_dir,f) for f in os.listdir(data_dir) if re.search("\.json",f)]
    countries = [os.path.join(data_dir,"ts-"+re.sub("\s+","_",c.lower()) + ".csv") for c in top_ten_countries]

    ## load the data
    dfs = {}
    dfs['all'] = convert_to_ts(df)
    for country in top_ten_countries:
        country_id = re.sub("\s+","_",country.lower())
        file_name = os.path.join(data_dir,"ts-"+ country_id + ".csv")
        dfs[country_id] = convert_to_ts(df,country=country)

    ## save the data as csvs    
    for key, item in dfs.items():
        item.to_csv(os.path.join(ts_data_dir,"ts-"+key+".csv"),index=False)
        
    return(dfs)

def engineer_features(df,training=True):
    """
    for any given day the target becomes the sum of the next days revenue
    for that day we engineer several features that help predict the summed revenue
    
    the 'training' flag will trim data that should not be used for training
    when set to false all data will be returned

    """

    ## extract dates
    dates = df['date'].values.copy()
    dates = dates.astype('datetime64[D]')

    ## engineer some features
    eng_features = defaultdict(list)
    previous =[7, 70]  #[7, 14, 21, 28, 35, 42, 49, 56, 63, 70]
    y = np.zeros(dates.size)
    for d,day in enumerate(dates):

        ## use windows in time back from a specific date
        for num in previous:
            current = np.datetime64(day, 'D') 
            prev = current - np.timedelta64(num, 'D')
            mask = np.in1d(dates, np.arange(prev,current,dtype='datetime64[D]'))
            eng_features["previous_{}".format(num)].append(df[mask]['revenue'].sum())

        ## get get the target revenue    
        plus_30 = current + np.timedelta64(30,'D')
        mask = np.in1d(dates, np.arange(current,plus_30,dtype='datetime64[D]'))
        y[d] = df[mask]['revenue'].sum()

        ## attempt to capture monthly trend with previous years data (if present)
        start_date = current - np.timedelta64(365,'D')
        stop_date = plus_30 - np.timedelta64(365,'D')
        mask = np.in1d(dates, np.arange(start_date,stop_date,dtype='datetime64[D]'))
        eng_features['previous_year'].append(df[mask]['revenue'].sum())

        ## add some non-revenue features
        minus_30 = current - np.timedelta64(30,'D')
        mask = np.in1d(dates, np.arange(minus_30,current,dtype='datetime64[D]'))
        eng_features['recent_invoices'].append(df[mask]['unique_invoices'].mean())
        eng_features['recent_views'].append(df[mask]['total_views'].mean())

    X = pd.DataFrame(eng_features)
    ## combine features in to df and remove rows with all zeros
    X.fillna(0,inplace=True)
    mask = X.sum(axis=1)>0
    X = X[mask]
    y = y[mask]
    dates = dates[mask]
    X.reset_index(drop=True, inplace=True)

    if training == True:
        ## remove the last 30 days (because the target is not reliable)
        mask = np.arange(X.shape[0]) < np.arange(X.shape[0])[-30]
        X = X[mask]
        y = y[mask]
        dates = dates[mask]
        X.reset_index(drop=True, inplace=True)
        


#    print(X)

#    dfplot1 = X[['previous_7']].copy()
#    dfplot2 = X[['previous_70']].copy()
#    dfplot3 = X[['previous_year']].copy()
    
#    fig = plt.figure(figsize=(10.0, 8.0))
#    ax = fig.add_subplot(111)
#    plt.plot(dfplot1, "r", label="pre7days")
#    plt.plot(dfplot2, "g", label="pre70days")
#    plt.plot(dfplot3, "b", label="pre1year")
#    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=18)
#    plt.show()

    return(X,y,dates)

